{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "adfd7dbe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch shapes: torch.Size([64, 20]) torch.Size([64, 16])\n",
      "Device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|█                                        | 1/40 [02:04<1:20:54, 124.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40, Loss: 2.271122564660742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█████▏                                   | 5/40 [10:40<1:15:56, 130.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/40, Loss: 1.0798931451553995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|█████████▏                               | 9/40 [23:22<1:32:11, 178.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/40, Loss: 0.16374219906456927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|█████████████                           | 13/40 [33:38<1:08:14, 151.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/40, Loss: 0.025988549865940784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|█████████████████▊                        | 17/40 [39:45<40:39, 106.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/40, Loss: 0.017777425613492093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|██████████████████████▌                    | 21/40 [45:54<30:15, 95.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/40, Loss: 0.014749614383153458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████████████████████████▉                | 25/40 [51:58<23:05, 92.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/40, Loss: 0.013067327241631264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████████████████████████████▏           | 29/40 [58:03<16:46, 91.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/40, Loss: 0.01263407063967687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|█████████████████████████████████▊       | 33/40 [1:04:08<10:40, 91.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/40, Loss: 0.012071249828218146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████████████████████████████████▉   | 37/40 [1:10:16<04:35, 91.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/40, Loss: 0.011943061900463826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 40/40 [1:14:50<00:00, 112.27s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import unicodedata\n",
    "import string\n",
    "\n",
    "# Set random seed for PyTorch CPU operations\n",
    "torch.manual_seed(42)\n",
    "\n",
    "\n",
    "# Preprocessing functions\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "def clean_text(text):\n",
    "    text = unicode_to_ascii(text.lower().strip())\n",
    "    text = re.sub(r\"i'm\", \"i am\", text)\n",
    "    text = re.sub(r\"\\r\", \"\", text)\n",
    "    text = re.sub(r\"he's\", \"he is\", text)\n",
    "    text = re.sub(r\"she's\", \"she is\", text)\n",
    "    text = re.sub(r\"it's\", \"it is\", text)\n",
    "    text = re.sub(r\"that's\", \"that is\", text)\n",
    "    text = re.sub(r\"what's\", \"that is\", text)\n",
    "    text = re.sub(r\"where's\", \"where is\", text)\n",
    "    text = re.sub(r\"how's\", \"how is\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"won't\", \"will not\", text)\n",
    "    text = re.sub(r\"can't\", \"cannot\", text)\n",
    "    text = re.sub(r\"n't\", \" not\", text)\n",
    "    text = re.sub(r\"n'\", \"ng\", text)\n",
    "    text = re.sub(r\"'bout\", \"about\", text)\n",
    "    text = re.sub(r\"'til\", \"until\", text)\n",
    "    text = re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,]\", \"\", text)\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation)) \n",
    "    text = re.sub(\"(\\\\W)\",\" \",text) \n",
    "    text = re.sub('\\S*\\d\\S*\\s*','', text)\n",
    "    text =  \"<sos> \" +  text + \" <eos>\"\n",
    "    \n",
    "    return text\n",
    "    \n",
    "    \n",
    "\n",
    "# Custom Dataset class\n",
    "class DialogDataset(Dataset):\n",
    "    def __init__(self, questions, answers, src_vocab, tgt_vocab):\n",
    "        self.questions = questions\n",
    "        self.answers = answers\n",
    "        self.src_vocab = src_vocab\n",
    "        self.tgt_vocab = tgt_vocab\n",
    "        self.tokenizer = get_tokenizer('basic_english')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.questions)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src = [self.src_vocab[token] for token in self.tokenizer(self.questions[idx])]\n",
    "        tgt = [self.tgt_vocab[token] for token in self.tokenizer(self.answers[idx])]\n",
    "        return torch.tensor(src), torch.tensor(tgt)\n",
    "\n",
    "    @staticmethod\n",
    "    def collate_fn(batch):\n",
    "        src_batch, tgt_batch = [], []\n",
    "        for src_item, tgt_item in batch:\n",
    "            src_batch.append(torch.cat([torch.tensor([src_vocab[\"<sos>\"]]), src_item, torch.tensor([src_vocab[\"<eos>\"]])], dim=0))\n",
    "            tgt_batch.append(torch.cat([torch.tensor([tgt_vocab[\"<sos>\"]]), tgt_item, torch.tensor([tgt_vocab[\"<eos>\"]])], dim=0))\n",
    "        src_batch = pad_sequence(src_batch, padding_value=src_vocab[\"<pad>\"]).transpose(0, 1)\n",
    "        tgt_batch = pad_sequence(tgt_batch, padding_value=tgt_vocab[\"<pad>\"]).transpose(0, 1)\n",
    "        return src_batch, tgt_batch\n",
    "\n",
    "# Load and preprocess data\n",
    "data = pd.read_csv(\"./dialogs.txt\", sep='\\t', header=None, names=['question', 'answer'])\n",
    "data[\"question\"] = data.question.apply(clean_text)\n",
    "data[\"answer\"] = data.answer.apply(clean_text)\n",
    "\n",
    "# Split data\n",
    "train_data, val_data = train_test_split(data, test_size=0.2)\n",
    "\n",
    "# Build vocabularies\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "\n",
    "def build_vocab(data):\n",
    "    vocab = build_vocab_from_iterator(map(tokenizer, data), specials=[\"<pad>\", \"<sos>\", \"<eos>\"])\n",
    "    vocab.set_default_index(vocab[\"<pad>\"])\n",
    "    return vocab\n",
    "\n",
    "\n",
    "src_vocab = build_vocab(train_data['question'])\n",
    "tgt_vocab = build_vocab(train_data['answer'])\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = DialogDataset(train_data['question'].tolist(), train_data['answer'].tolist(), src_vocab, tgt_vocab)\n",
    "val_dataset = DialogDataset(val_data['question'].tolist(), val_data['answer'].tolist(), src_vocab, tgt_vocab)\n",
    "\n",
    "# DataLoader\n",
    "BATCH_SIZE = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=DialogDataset.collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=DialogDataset.collate_fn)\n",
    "\n",
    "\n",
    "# checking dimension batch from DataLoader\n",
    "for src, tgt in train_loader:\n",
    "    print(\"Batch shapes:\", src.shape, tgt.shape)\n",
    "    break\n",
    "from torch import nn\n",
    "\n",
    "# Set random seed for PyTorch CPU operations\n",
    "torch.manual_seed(42)\n",
    "device = torch.device('cpu') \n",
    "print('Device:',device)\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = nn.GRU(embedding_dim, enc_units, batch_first=True)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, state = self.gru(x, hidden)\n",
    "        return output, state\n",
    "\n",
    "    def initialize_hidden_state(self, batch_size):\n",
    "        return torch.zeros((1, batch_size, self.enc_units))\n",
    "\n",
    "\n",
    "# Parameters for the model\n",
    "embedding_dim = 256\n",
    "units = 1024\n",
    "vocab_inp_size = len(src_vocab)\n",
    "vocab_tar_size = len(tgt_vocab)\n",
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE).to(device)\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BahdanauAttention(nn.Module):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = nn.Linear(units, units)\n",
    "        self.W2 = nn.Linear(units, units)\n",
    "        self.V = nn.Linear(units, 1)\n",
    "\n",
    "    def forward(self, query, values):\n",
    "        query_with_time_axis = query.unsqueeze(1)  # Shape: [batch_size, 1, hidden_size]\n",
    "        query_layer = self.W1(query_with_time_axis)  # Shape: [batch_size, 1, hidden_size]\n",
    "        values_layer = self.W2(values)  # Shape: [batch_size, max_len, hidden_size]\n",
    "\n",
    "        #print(\"query_layer shape:\", query_layer.shape)\n",
    "        #print(\"values_layer shape:\", values_layer.shape)\n",
    "\n",
    "        # Broadcasting query_layer to match the shape of values_layer\n",
    "        query_layer = query_layer.expand_as(values_layer)\n",
    "\n",
    "        # Calculate the score\n",
    "        score = self.V(torch.tanh(query_layer + values_layer))  # Shape: [batch_size, max_length, 1]\n",
    "        attention_weights = F.softmax(score, dim=1)\n",
    "\n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = torch.sum(context_vector, dim=1)\n",
    "\n",
    "        return context_vector, attention_weights\n",
    "\n",
    "\n",
    "# Initialize the attention layer\n",
    "attention_layer = BahdanauAttention(units)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = nn.GRU(embedding_dim + dec_units, dec_units, batch_first=True)\n",
    "        self.fc = nn.Linear(dec_units, vocab_size)\n",
    "\n",
    "        # used for attention\n",
    "        self.attention = BahdanauAttention(dec_units)\n",
    "\n",
    "    def forward(self, x, hidden, enc_output):\n",
    "        # Ensure hidden state is 2D [batch_size, hidden_size]\n",
    "        if hidden.dim() == 3:\n",
    "            hidden = hidden.squeeze(0)  # Removes the first dimension if it's of size 1\n",
    "\n",
    "        # Attention layer\n",
    "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "\n",
    "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # Concatenate context vector and x\n",
    "        x = torch.cat((context_vector.unsqueeze(1), x), -1)\n",
    "\n",
    "        # Passing the concatenated vector to the GRU\n",
    "        output, state = self.gru(x, hidden.unsqueeze(0))\n",
    "\n",
    "        # output shape == (batch_size, 1, hidden_size)\n",
    "        output = output.reshape(-1, output.size(2))\n",
    "\n",
    "        # output shape == (batch_size, vocab)\n",
    "        x = self.fc(output)\n",
    "\n",
    "        return x, state.squeeze(0), attention_weights\n",
    "\n",
    "    \n",
    "# Initialize the decoder\n",
    "\n",
    "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE).to(device)\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=0.001)\n",
    "\n",
    "# Custom Loss Function\n",
    "def loss_function(real, pred):\n",
    "    # Mask for non-zero tokens in the target\n",
    "    mask = real.ne(0)\n",
    "    loss = F.cross_entropy(pred, real, reduction='none')\n",
    "    loss = loss * mask\n",
    "    return loss.mean()\n",
    "\n",
    "# Training Step Function\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "    # Move data to the device\n",
    "    inp, targ, enc_hidden = inp.to(device), targ.to(device), enc_hidden.to(device)\n",
    "\n",
    "    loss = 0\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    current_batch_size = inp.size(0)\n",
    "    enc_hidden = enc_hidden[:, :current_batch_size, :]\n",
    "\n",
    "    enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "    dec_hidden = enc_hidden\n",
    "\n",
    "    sos_token_index = tgt_vocab['<sos>']\n",
    "    dec_input = torch.full((current_batch_size, 1), sos_token_index, dtype=torch.long, device=inp.device)\n",
    "\n",
    "    for t in range(1, targ.size(1)):\n",
    "        predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "        loss += loss_function(targ[:, t], predictions.squeeze(1))\n",
    "        dec_input = targ[:, t].unsqueeze(1)\n",
    "\n",
    "    batch_loss = loss / int(targ.size(1))\n",
    "    batch_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return batch_loss.item()\n",
    "\n",
    "# Training Loop\n",
    "EPOCHS = 40\n",
    "for epoch in tqdm(range(EPOCHS)):\n",
    "    total_loss = 0\n",
    "\n",
    "    for inp, targ in train_loader:\n",
    "        current_batch_size = inp.size(0)\n",
    "        # Initialize hidden state with the correct current batch size\n",
    "        enc_hidden = encoder.initialize_hidden_state(current_batch_size).to(device)\n",
    "\n",
    "        \n",
    "        batch_loss = train_step(inp, targ, enc_hidden)\n",
    "        total_loss += batch_loss\n",
    "\n",
    "    if epoch % 4 == 0:\n",
    "        print(f'Epoch {epoch + 1}/{EPOCHS}, Loss: {total_loss / len(train_loader)}')\n",
    "\n",
    "        \n",
    "        \n",
    "#save the model \n",
    "torch.save(encoder.state_dict(), 'encoder.pth')\n",
    "torch.save(decoder.state_dict(), 'decoder.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9a33cea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Question: <sos> good luck with school <eos>\n",
      "Predicted answer: <sos> i think you should go to go \n"
     ]
    }
   ],
   "source": [
    "max_length_targ = max(len(t.split()) for t in train_data['answer'])\n",
    "\n",
    "def evaluate(sentence):\n",
    "    sentence = clean_text(sentence)\n",
    "\n",
    "    inputs = [src_vocab[token] for token in sentence.split(' ')]\n",
    "    inputs = torch.tensor([inputs]).to(device)\n",
    "\n",
    "    result = ''\n",
    "\n",
    "    # Initialize the hidden state with zeros\n",
    "    hidden = torch.zeros((1, 1, units)).to(device)  # Modify the shape according to your GRU layer\n",
    "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "\n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = torch.tensor([[tgt_vocab['<sos>']]], dtype=torch.long).to(device)\n",
    "\n",
    "    for t in range(max_length_targ):\n",
    "        predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_out)\n",
    "\n",
    "        predicted_id = torch.argmax(predictions[0]).item()\n",
    "\n",
    "        # Reverse lookup function\n",
    "        def index_to_word(vocab, index):\n",
    "            return vocab.get_itos()[index]\n",
    "\n",
    "        if index_to_word(tgt_vocab, predicted_id) == '<eos>':\n",
    "            break\n",
    "\n",
    "        result += index_to_word(tgt_vocab, predicted_id) + ' '\n",
    "\n",
    "        # The predicted ID is fed back into the model\n",
    "        dec_input = torch.tensor([[predicted_id]], dtype=torch.long).to(device)\n",
    "\n",
    "    return result, sentence\n",
    "\n",
    "\n",
    "# Example usage\n",
    "def ask(sentence):\n",
    "    result, sentence = evaluate(sentence)\n",
    "\n",
    "    print('Question: %s' % (sentence))\n",
    "    print('Predicted answer: {}'.format(result))\n",
    "\n",
    "# Load questions and answers from a file\n",
    "questions = []\n",
    "answers = []\n",
    "with open(\"./dialogs.txt\", 'r') as f:\n",
    "    for line in f:\n",
    "        line = line.split('\\t')\n",
    "        questions.append(line[0])\n",
    "        answers.append(line[1])\n",
    "\n",
    "print(len(questions) == len(answers))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2c8a41ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i'm actually in school right now.\n",
      "\n",
      "i've actually been pretty good. you?\n"
     ]
    }
   ],
   "source": [
    "print(answers[15])\n",
    "print(questions[15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ed3e7435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: <sos> it is not bad there are a lot of people there <eos>\n",
      "Predicted answer: <sos> good luck with that \n",
      "None\n",
      "good luck with that.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example usage with a specific question\n",
    "print(ask(questions[20]))\n",
    "print(answers[20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "643084a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type your question (or 'exit' to quit): How are you?\n",
      "Model's answer: ('<sos> i am fine how about yourself ', '<sos> how are you <eos>')\n",
      "\n",
      "\n",
      "Type your question (or 'exit' to quit): who are you?\n",
      "Model's answer: ('<sos> i am not sure ', '<sos> who are you <eos>')\n",
      "\n",
      "\n",
      "Type your question (or 'exit' to quit): can you help me ?\n",
      "Model's answer: ('<sos> what time does mi mean ', '<sos> can you help me  <eos>')\n",
      "\n",
      "\n",
      "Type your question (or 'exit' to quit): Hi, there are a lot of people here.\n",
      "Model's answer: ('<sos> there are lots of police ', '<sos> hi there are a lot of people here <eos>')\n",
      "\n",
      "\n",
      "Type your question (or 'exit' to quit): exit\n"
     ]
    }
   ],
   "source": [
    "# Function to interactively ask questions and get answers\n",
    "def interact_with_model():\n",
    "    while True:\n",
    "        # Get user input\n",
    "        user_input = input(\"Type your question (or 'exit' to quit): \")\n",
    "\n",
    "        # Check if the user wants to exit\n",
    "        if user_input.lower() == 'exit':\n",
    "            break\n",
    "\n",
    "        # Get the model's answer\n",
    "        answer = evaluate(user_input)\n",
    "\n",
    "        # Display the model's answer\n",
    "        print(\"Model's answer:\", answer)\n",
    "        print(\"\\n\")\n",
    "\n",
    "# Start the interactive loop\n",
    "interact_with_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4b873a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
