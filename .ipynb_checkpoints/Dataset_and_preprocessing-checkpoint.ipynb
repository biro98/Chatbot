{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e88e4982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch shapes: torch.Size([64, 19]) torch.Size([64, 19])\n",
      "2090\n",
      "2137\n",
      "Max index in source batch: 2084\n",
      "Max index in target batch: 1990\n",
      "Max index in source batch: 2007\n",
      "Max index in target batch: 2061\n",
      "Max index in source batch: 2067\n",
      "Max index in target batch: 2131\n",
      "Max index in source batch: 2010\n",
      "Max index in target batch: 2081\n",
      "Max index in source batch: 2031\n",
      "Max index in target batch: 2119\n",
      "Max index in source batch: 2068\n",
      "Max index in target batch: 1887\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import unicodedata\n",
    "import string\n",
    "\n",
    "# Set random seed for PyTorch CPU operations\n",
    "torch.manual_seed(42)\n",
    "\n",
    "\n",
    "# Preprocessing functions\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "def clean_text(text):\n",
    "    text = unicode_to_ascii(text.lower().strip())\n",
    "    text = re.sub(r\"i'm\", \"i am\", text)\n",
    "    text = re.sub(r\"\\r\", \"\", text)\n",
    "    text = re.sub(r\"he's\", \"he is\", text)\n",
    "    text = re.sub(r\"she's\", \"she is\", text)\n",
    "    text = re.sub(r\"it's\", \"it is\", text)\n",
    "    text = re.sub(r\"that's\", \"that is\", text)\n",
    "    text = re.sub(r\"what's\", \"that is\", text)\n",
    "    text = re.sub(r\"where's\", \"where is\", text)\n",
    "    text = re.sub(r\"how's\", \"how is\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"won't\", \"will not\", text)\n",
    "    text = re.sub(r\"can't\", \"cannot\", text)\n",
    "    text = re.sub(r\"n't\", \" not\", text)\n",
    "    text = re.sub(r\"n'\", \"ng\", text)\n",
    "    text = re.sub(r\"'bout\", \"about\", text)\n",
    "    text = re.sub(r\"'til\", \"until\", text)\n",
    "    text = re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,]\", \"\", text)\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation)) \n",
    "    text = re.sub(\"(\\\\W)\",\" \",text) \n",
    "    text = re.sub('\\S*\\d\\S*\\s*','', text)\n",
    "    text =  \"<sos> \" +  text + \" <eos>\"\n",
    "    \n",
    "    return text\n",
    "    \n",
    "    \n",
    "\n",
    "# Custom Dataset class\n",
    "class DialogDataset(Dataset):\n",
    "    def __init__(self, questions, answers, src_vocab, tgt_vocab):\n",
    "        self.questions = questions\n",
    "        self.answers = answers\n",
    "        self.src_vocab = src_vocab\n",
    "        self.tgt_vocab = tgt_vocab\n",
    "        self.tokenizer = get_tokenizer('basic_english')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.questions)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src = [self.src_vocab[token] for token in self.tokenizer(self.questions[idx])]\n",
    "        tgt = [self.tgt_vocab[token] for token in self.tokenizer(self.answers[idx])]\n",
    "        return torch.tensor(src), torch.tensor(tgt)\n",
    "\n",
    "    @staticmethod\n",
    "    def collate_fn(batch):\n",
    "        src_batch, tgt_batch = [], []\n",
    "        for src_item, tgt_item in batch:\n",
    "            src_batch.append(torch.cat([torch.tensor([src_vocab[\"<sos>\"]]), src_item, torch.tensor([src_vocab[\"<eos>\"]])], dim=0))\n",
    "            tgt_batch.append(torch.cat([torch.tensor([tgt_vocab[\"<sos>\"]]), tgt_item, torch.tensor([tgt_vocab[\"<eos>\"]])], dim=0))\n",
    "        src_batch = pad_sequence(src_batch, padding_value=src_vocab[\"<pad>\"]).transpose(0, 1)\n",
    "        tgt_batch = pad_sequence(tgt_batch, padding_value=tgt_vocab[\"<pad>\"]).transpose(0, 1)\n",
    "        return src_batch, tgt_batch\n",
    "\n",
    "# Load and preprocess data\n",
    "data = pd.read_csv(\"./dialogs.txt\", sep='\\t', header=None, names=['question', 'answer'])\n",
    "data[\"question\"] = data.question.apply(clean_text)\n",
    "data[\"answer\"] = data.answer.apply(clean_text)\n",
    "\n",
    "# Split data\n",
    "train_data, val_data = train_test_split(data, test_size=0.2)\n",
    "\n",
    "# Build vocabularies\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "\n",
    "def build_vocab(data):\n",
    "    vocab = build_vocab_from_iterator(map(tokenizer, data), specials=[\"<pad>\", \"<sos>\", \"<eos>\"])\n",
    "    vocab.set_default_index(vocab[\"<pad>\"])\n",
    "    return vocab\n",
    "\n",
    "\n",
    "src_vocab = build_vocab(train_data['question'])\n",
    "tgt_vocab = build_vocab(train_data['answer'])\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = DialogDataset(train_data['question'].tolist(), train_data['answer'].tolist(), src_vocab, tgt_vocab)\n",
    "val_dataset = DialogDataset(val_data['question'].tolist(), val_data['answer'].tolist(), src_vocab, tgt_vocab)\n",
    "\n",
    "# DataLoader\n",
    "BATCH_SIZE = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=DialogDataset.collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=DialogDataset.collate_fn)\n",
    "\n",
    "\n",
    "# checking dimension batch from DataLoader\n",
    "for src, tgt in train_loader:\n",
    "    print(\"Batch shapes:\", src.shape, tgt.shape)\n",
    "    break\n",
    "\n",
    "#save vocabularies\n",
    "import pickle\n",
    "\n",
    "# Assuming src_vocab and tgt_vocab are your source and target vocabularies\n",
    "with open('src_vocab.pkl', 'wb') as f:\n",
    "    pickle.dump(src_vocab, f)\n",
    "\n",
    "with open('tgt_vocab.pkl', 'wb') as f:\n",
    "    pickle.dump(tgt_vocab, f)\n",
    "    \n",
    "print(len(src_vocab))\n",
    "print(len(tgt_vocab))\n",
    "\n",
    "\n",
    "#save train and val data\n",
    "train_data.to_pickle('train_data.pkl')\n",
    "val_data.to_pickle('val_data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65b8517",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9737d066",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
